{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "aab9a569",
      "metadata": {
        "id": "aab9a569"
      },
      "source": [
        "## Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b8190b19",
      "metadata": {
        "id": "b8190b19"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-06-15 07:56:58.832782: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "import io\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
        "from tensorflow.keras.layers import TextVectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5DvQNvnr02MC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DvQNvnr02MC",
        "outputId": "d47e32bd-a4b8-4e2c-e146-43e0d97096b9"
      },
      "outputs": [],
      "source": [
        "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "\n",
        "dataset = tf.keras.utils.get_file(\"aclImdb_v1.tar.gz\", url,\n",
        "                                  untar=True, cache_dir='.',\n",
        "                                  cache_subdir='')\n",
        "\n",
        "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
        "os.listdir(dataset_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "BA4T9l6R02OX",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BA4T9l6R02OX",
        "outputId": "0a25ad3d-5e98-434c-e52c-0c6c463c2e34"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['unsupBow.feat',\n",
              " 'urls_unsup.txt',\n",
              " 'unsup',\n",
              " 'urls_neg.txt',\n",
              " 'neg',\n",
              " 'labeledBow.feat',\n",
              " 'pos',\n",
              " 'urls_pos.txt']"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "train_dir = os.path.join(dataset_dir, 'train')\n",
        "os.listdir(train_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "Pjth0RHA02S4",
      "metadata": {
        "id": "Pjth0RHA02S4"
      },
      "outputs": [],
      "source": [
        "remove_dir = os.path.join(train_dir, 'unsup')\n",
        "shutil.rmtree(remove_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "fafaf36e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fafaf36e",
        "outputId": "8c602496-b8da-45a3-fcd1-59706cd4068e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Using 20000 files for training.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "Using 5000 files for validation.\n"
          ]
        }
      ],
      "source": [
        "batch_size = 1024\n",
        "seed = 12345\n",
        "train_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "                            'aclImdb/train', batch_size=batch_size, \n",
        "                            validation_split=0.2,\n",
        "                            subset='training', seed=seed)\n",
        "val_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "                            'aclImdb/train', batch_size=batch_size, \n",
        "                            validation_split=0.2,\n",
        "                            subset='validation', seed=seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "bUm-SqpGvkcV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUm-SqpGvkcV",
        "outputId": "95b168f4-af42-4139-b295-aae7479ad9f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n",
            "b\"The first von Trier movie i've ever seen was breaking the waves. Sure a nice movie but it definitely stands in the shadow of europa. Europa tells a story of a young German-American who wants to experience Germany just after the second world war. He takes a job that his uncle has arranged for him as a purser on a luxues train. Because of his job, he travels all through an almost totally destroyed germany, meeting with the killing of traitors, and hunt for former nazi party members. The society is suffering from corruption. His uncle has narrowed his conciousness by focussing on the job he has also as a purser on the train. By coincidence the main character get involved in bombing and terrorism by a group called 'werewolves' they put pressure on him to help them placing bombs on trains. The atmosphere is astounding. The viewer is taken from scene to scene by a man attempting to put the viewer under hypnosis and then counting to wake you up in a new scene. Just when you think you've seen a lot!!!!!!! europe!!\"\n"
          ]
        }
      ],
      "source": [
        "for text_batch, label_batch in train_ds:\n",
        "    print(label_batch[0].numpy())\n",
        "    print(text_batch.numpy()[0])\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "QhfNnrU0vqWu",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhfNnrU0vqWu",
        "outputId": "79df5875-188a-445d-8441-2887d6d1448a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "I read the comment of Chris_m_grant from United States.<br /><br />He wrote : \" A Fantastic documentary of 1924. This early 20th century geography of today's Iraq was powerful.\"<br /><br />I would like to thank Chris and people who are interested in Bakhtiari Nomads of Iran, the Zagros mountains and landscapes and have watched the movie Grass, A Nation's battle for life. These traditions you saw in the movie have endured for centuries and will go on as long as life endures. I am from this region of Iran myself. I am a Bakhtiari. <br /><br />Chris, I am sorry to bother you but Bakhtiari region of Zardkuh is in Iran not in Irak as you mentioned in your comment. Iran and Irak are two different and distinct countries. Taking an Iranian for an Irankian is almost like taking an American for an Mexican. Thanks,<br /><br />Ziba\n"
          ]
        }
      ],
      "source": [
        "for text_batch, label_batch in train_ds:\n",
        "    print(label_batch[0].numpy())\n",
        "    print(text_batch.numpy()[0].decode('ascii'))\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "fed87acf",
      "metadata": {
        "id": "fed87acf"
      },
      "outputs": [],
      "source": [
        "vocab_size   = 20000\n",
        "sequence_len = 200\n",
        "\n",
        "def custom_standardization(input_data):\n",
        "    lowercase = tf.strings.lower(input_data)\n",
        "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
        "    return tf.strings.regex_replace(\n",
        "        stripped_html, f\"[{re.escape(string.punctuation)}]\", \"\"\n",
        "    )\n",
        "\n",
        "vectorization = tf.keras.layers.TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_len,\n",
        ")\n",
        "\n",
        "vectorization.adapt(train_ds.map(lambda text, label: text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "7M6KMEWdwPiA",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7M6KMEWdwPiA",
        "outputId": "27027912-43f4-4760-cfe3-934d9b062632"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "i read the comment of chrismgrant from united states  he wrote   a fantastic documentary of 1924 this early 20th century geography of todays iraq was powerful  i would like to thank chris and people who are interested in bakhtiari nomads of iran the zagros mountains and landscapes and have watched the movie grass a nations battle for life these traditions you saw in the movie have endured for centuries and will go on as long as life endures i am from this region of iran myself i am a bakhtiari   chris i am sorry to bother you but bakhtiari region of zardkuh is in iran not in irak as you mentioned in your comment iran and irak are two different and distinct countries taking an iranian for an irankian is almost like taking an american for an mexican thanks  ziba\n"
          ]
        }
      ],
      "source": [
        "output = custom_standardization(text_batch.numpy()[0].decode('ascii'))\n",
        "print(output.numpy().decode())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "eaa2d5a2",
      "metadata": {
        "id": "eaa2d5a2"
      },
      "outputs": [],
      "source": [
        "def vectorize_text(text, label):\n",
        "    text = tf.expand_dims(text, -1)\n",
        "    return vectorization(text), label\n",
        "\n",
        "train_ds = train_ds.map(vectorize_text)\n",
        "val_ds = val_ds.map(vectorize_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "2486508b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2486508b",
        "outputId": "e7247c5c-9623-4e19-f931-7f8b0c73f6c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n",
            "[   44    22    38   330     1   209    66   115    34   182    30    65\n",
            "   971     3  1279  5960    29    55   413   284   498    91     5    30\n",
            "   208     1     3     1     2  2978     5     2  3438   818  5742     2\n",
            " 10008     7    21  8070    32     1     1     4   989   299  3034     3\n",
            "    40    81     9    13    90     8 10586    89  2152    22    68    66\n",
            "     2  5511  2388     2  1511 17410    56    11    13     2    84    17\n",
            "   115    22   400  4328     9    13    90    31     1  2525     1     3\n",
            "  5099    31     2    58    64     1  2643   421     9     3   172     1\n",
            "   928   633    13 12271   209   339   360    10   585    86    56    10\n",
            "  5366  1829 12960    13     4 14897    15     4   172    36    13   457\n",
            "    41   299   837   779    18    31     2    58   152   178     6    27\n",
            "  3321    16   927    61    13  3376     4   958   521   141    92    11\n",
            "    13   337     4    52    52 12062   354    41     4    52     1  1265\n",
            "    20     2   171     5     2  1493    36     1   351    47    24     2\n",
            "     1     1    16   423   215   216     1  7772    12  3034   325 17901\n",
            "    47    24  1080  3686    12 11315     2  2701  4452 17753  6152    14\n",
            "  1928   209     6    27   969    72   323     1]\n"
          ]
        }
      ],
      "source": [
        "for text_batch, label_batch in train_ds:\n",
        "    print(label_batch[0].numpy())\n",
        "    print(text_batch.numpy()[0])\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "2a8a2783",
      "metadata": {
        "id": "2a8a2783"
      },
      "outputs": [],
      "source": [
        "train_ds = train_ds.cache().prefetch(buffer_size=10)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=10)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "5484718e",
      "metadata": {
        "id": "5484718e"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "cffe32c7",
      "metadata": {
        "id": "cffe32c7"
      },
      "outputs": [],
      "source": [
        "\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super().__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate) \n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "1a727afd",
      "metadata": {
        "id": "1a727afd"
      },
      "outputs": [],
      "source": [
        " # Two seperate embedding layers, one for tokens, one for token index (positions)\n",
        "\n",
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super().__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "68667876",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68667876",
        "outputId": "5fe287ed-69ef-48be-9bc7-023adae3ea35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 200)]             0         \n",
            "                                                                 \n",
            " token_and_position_embeddin  (None, 200, 128)         2585600   \n",
            " g (TokenAndPositionEmbeddin                                     \n",
            " g)                                                              \n",
            "                                                                 \n",
            " transformer_block (Transfor  (None, 200, 128)         429184    \n",
            " merBlock)                                                       \n",
            "                                                                 \n",
            " global_average_pooling1d (G  (None, 128)              0         \n",
            " lobalAveragePooling1D)                                          \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 32)                4128      \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 32)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 2)                 66        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,018,978\n",
            "Trainable params: 3,018,978\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "embed_dim = 128  # Embedding size for each token\n",
        "num_heads = 6    # Number of attention heads\n",
        "ff_dim = 128     # Hidden layer size in feed forward network inside transformer\n",
        "\n",
        "inputs = layers.Input(shape=(sequence_len,))\n",
        "embedding_layer = TokenAndPositionEmbedding(sequence_len, vocab_size, embed_dim)\n",
        "x = embedding_layer(inputs)\n",
        "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "x = transformer_block(x)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "x = layers.Dense(32, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cd01ada",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cd01ada",
        "outputId": "4ef8279f-c551-462d-9fc7-2ec8280a11c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "20/20 [==============================] - 25s 641ms/step - loss: 0.7280 - accuracy: 0.5276 - val_loss: 0.6552 - val_accuracy: 0.6600\n",
            "Epoch 2/3\n",
            "20/20 [==============================] - 11s 564ms/step - loss: 0.5462 - accuracy: 0.7181 - val_loss: 0.3922 - val_accuracy: 0.8270\n",
            "Epoch 3/3\n",
            "20/20 [==============================] - 11s 549ms/step - loss: 0.3184 - accuracy: 0.8653 - val_loss: 0.3348 - val_accuracy: 0.8606\n"
          ]
        }
      ],
      "source": [
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "history = model.fit(train_ds, batch_size=32, epochs=3, validation_data=val_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PWWlaVTfe0WW",
      "metadata": {
        "id": "PWWlaVTfe0WW"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
