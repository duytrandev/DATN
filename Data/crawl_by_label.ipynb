{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = \"https://baochinhphu.vn\"\n",
    "# page_index_string = \"/the-thao/trang\"\n",
    "uri = \"/chinh-tri.htm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_news = []\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "driver.get(domain + uri)\n",
    "time.sleep(1)\n",
    "\n",
    "for _ in range(10):\n",
    "  for _ in range(7):\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    all_tag = soup.find_all('a', class_= 'box-stream-link-with-avatar')\n",
    "    print(len(all_tag))\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(5)\n",
    "  time.sleep(2)\n",
    "  btn_xemthem = driver.find_elements(By.CLASS_NAME, \"btn\")[1]\n",
    "  if btn_xemthem:\n",
    "    btn_xemthem.click()\n",
    "  # print(btn_xemthem)\n",
    "  # break\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_link = [domain + i.get(\"href\") for i in all_tag]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_link_politics = all_link\n",
    "contents_politics = contents\n",
    "labels_polittics = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "driver.implicitly_wait(10)\n",
    "for i in range(len(all_link)):\n",
    "  try: \n",
    "    driver.get(all_link[i])\n",
    "    time.sleep(1)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    content_element = soup.find(\"div\", class_= \"detail-content\")\n",
    "    # filtered_content_element = [elem for elem in content_element if not elem.has_attr('class') or 'bm_IO' not in elem.get('class')]\n",
    "    content_string = \" \".join([i.text for i in content_element.find_all(\"p\")[1:]])\n",
    "    label_element = \"politics\"\n",
    "    # if content_string in contents: \n",
    "    #   print(contents.index(content_string))\n",
    "    #   print(url_news.index(content_string))\n",
    "    #   print(link)\n",
    "    #   continue\n",
    "    contents.append(content_string)\n",
    "    labels.append(label_element)\n",
    "  except:\n",
    "    print(i)\n",
    "    contents.append(\"\")\n",
    "    labels.append(\"\")\n",
    "    driver.quit()\n",
    "    driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chinhtri = pd.DataFrame({\"Link\": all_link_politics, \"Content\": contents_politics, \"Label\": labels_polittics})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chinhtri.to_csv(\"politics_baochinhphu.csv\", encoding= \"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_chinhtri)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# giao duc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_giaoduc = \"https://giaoduc.net.vn\"\n",
    "uri_giaoduc = \"/giao-duc-24h/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_news_giaoduc = []\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "driver.get(domain_giaoduc + uri_giaoduc)\n",
    "for _ in range(20):\n",
    "  for _ in range(5):\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    all_tag = soup.find_all('a', class_= 'story__title')\n",
    "    print(len(all_tag))\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(3)\n",
    "  time.sleep(2)\n",
    "  btn_xemthem = driver.find_elements(By.CLASS_NAME, \"next__page\")[0]\n",
    "  print(btn_xemthem)\n",
    "  if btn_xemthem:\n",
    "    btn_xemthem.click()\n",
    "  # print(btn_xemthem)\n",
    "  # break\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_link_educations = []\n",
    "content_educations = []\n",
    "label_educations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in all_tag:\n",
    "  if i.get(\"href\")[:20] != domain_giaoduc[:20]:\n",
    "    all_link_educations.append(domain_giaoduc + i.get(\"href\"))\n",
    "  else:\n",
    "    all_link_educations.append(i.get(\"href\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "driver.implicitly_wait(10)\n",
    "for i in range(len(all_link_educations)):\n",
    "  try: \n",
    "    driver.get(all_link_educations[i])\n",
    "    time.sleep(1)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    content_element = soup.find(\"div\", class_= \"details__content\")\n",
    "    # filtered_content_element = [elem for elem in content_element if not elem.has_attr('class') or 'bm_IO' not in elem.get('class')]\n",
    "    content_string = \" \".join([i.text for i in content_element.find_all(\"p\")])\n",
    "    \n",
    "    label_element = \"education\"\n",
    "    # if content_string in contents: \n",
    "    #   print(contents.index(content_string))\n",
    "    #   print(url_news.index(content_string))\n",
    "    #   print(link)\n",
    "    #   continue\n",
    "    content_educations.append(content_string)\n",
    "    label_educations.append(label_element)\n",
    "  except:\n",
    "    print(i)\n",
    "    content_educations.append(\"\")\n",
    "    label_educations.append(\"\")\n",
    "    driver.quit()\n",
    "    driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_education = pd.DataFrame({\"Link\": all_link_educations, \"Content\": content_educations, \"Label\": label_educations})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_link_educations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_education.to_csv(\"education_giaoducnet.csv\", encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_sport = \"https://thethao247.vn\"\n",
    "uri_sport = \"/the-thao-tong-hop-c5/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_news_sport = []\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "driver.get(domain_sport + uri_sport)\n",
    "for _ in range(20):\n",
    "  for _ in range(3):\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    all_tag = soup.find_all('a', class_= 'thumb')\n",
    "    print(len(all_tag))\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(5)\n",
    "  time.sleep(5)\n",
    "  btn_xemthem = driver.find_elements(By.CLASS_NAME, \"btn_loadmore\")[0]\n",
    "  print(btn_xemthem)\n",
    "  if btn_xemthem:\n",
    "    btn_xemthem.click()\n",
    "  # print(btn_xemthem)\n",
    "  # break\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_sports = []\n",
    "content_sports = []\n",
    "label_sports = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "driver.implicitly_wait(10)\n",
    "for i in range(len(link_sports)):\n",
    "  try: \n",
    "    driver.get(link_sports[i])\n",
    "    time.sleep(1)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    content_element = soup.find(\"div\", class_= \"details__content\")\n",
    "    # filtered_content_element = [elem for elem in content_element if not elem.has_attr('class') or 'bm_IO' not in elem.get('class')]\n",
    "    content_string = \" \".join([i.text for i in content_element.find_all(\"p\")])\n",
    "    \n",
    "    label_element = \"sport\"\n",
    "    # if content_string in contents: \n",
    "    #   print(contents.index(content_string))\n",
    "    #   print(url_news.index(content_string))\n",
    "    #   print(link)\n",
    "    #   continue\n",
    "    content_sports.append(content_string)\n",
    "    label_sports.append(label_element)\n",
    "  except:\n",
    "    print(i)\n",
    "    content_sports.append(\"\")\n",
    "    label_sports.append(\"\")\n",
    "    driver.quit()\n",
    "    driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "driver.quit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# khoa hoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = \"https://khoahoc.tv\"\n",
    "uri = \"/cong-nghe?p=97\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_content = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "for _ in range(60): \n",
    "  driver.get(link+uri)    \n",
    "  time.sleep(2)\n",
    "  soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "  for i in soup.find_all('a', class_= 'title'):\n",
    "    links_content.append(i.get(\"href\"))\n",
    "  print(len(set(links_content)))\n",
    "  uri = soup.find(\"li\", class_= \"PagedList-skipToNext\").find(\"a\").get(\"href\")\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_content = [(link+x) for x in list(set(links_content)) if link not in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(links_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contents = []\n",
    "# labels = []\n",
    "# links = []\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "for l in links_content[1316:]: \n",
    "  driver.get(l)    \n",
    "  time.sleep(2)\n",
    "  soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "  all_p = soup.find('div', class_= \"content-detail textview\").find_all(\"p\")\n",
    "  content = \" \".join([x.text for x in all_p if len(x.text.split()) > 10])\n",
    "  label = \"Khoa học\"\n",
    "  labels.append(label)\n",
    "  contents.append(content)\n",
    "  links.append(l)\n",
    "  # print(len(labels))\n",
    "  # try:\n",
    "  #   driver.get(l)    \n",
    "  #   time.sleep(2)\n",
    "  #   soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "  #   all_p = soup.find('div', class_= \"content-detail textview\").find_all(\"p\")\n",
    "  #   content = \" \".join([x.text for x in all_p if len(x.text.split()) > 10])\n",
    "  #   label = \"Khoa học\"\n",
    "  # except:\n",
    "  #   driver.quit()\n",
    "  #   driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "  #   content = \"\"\n",
    "  #   label = \"\"\n",
    "  # labels.append(label)\n",
    "  # contents.append(content)\n",
    "  # links.append(l)\n",
    "  print(len(labels))\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"Link\": links, 'Content': contents, 'Label': labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"khoahoc.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_content[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# van hoa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_url_vanhoa = \"https://laodong.vn\"\n",
    "temp = \"/van-hoa\"\n",
    "uri_vanhoa = \"?page=3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_vanhoa = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3p/99_58tl56kzdbsb1dw6qrs3m0000gn/T/ipykernel_71653/3591919021.py:1: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(ChromeDriverManager().install())\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "for _ in range(20): \n",
    "  driver.get(link_url_vanhoa + temp + uri_vanhoa)    \n",
    "  time.sleep(2)\n",
    "  soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "  for i in soup.find_all('a', class_= 'link-title'):\n",
    "    links_vanhoa.append(i.get(\"href\"))\n",
    "  uri_vanhoa = soup.find_all(\"a\", class_= \"page-link\")[-2].get('href')\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1866, 1866)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links_vanhoa = list(set(links_vanhoa))\n",
    "links_vanhoa1 = [(link+x) if link not in x else x for x in (links_vanhoa) ]\n",
    "len(links_vanhoa1), len(links_vanhoa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents_vanhoa = []\n",
    "labels_vanhoa = []\n",
    "links_vanhoa = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3p/99_58tl56kzdbsb1dw6qrs3m0000gn/T/ipykernel_71653/3255716546.py:1: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(ChromeDriverManager().install())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1675\n",
      "1676\n",
      "1677\n",
      "1678\n",
      "1679\n",
      "1680\n",
      "1681\n",
      "1682\n",
      "1683\n",
      "1684\n",
      "1685\n",
      "1686\n",
      "1687\n",
      "1688\n",
      "1689\n",
      "1690\n",
      "1691\n",
      "1692\n",
      "1693\n",
      "1694\n",
      "1695\n",
      "1696\n",
      "1697\n",
      "1698\n",
      "1699\n",
      "1700\n",
      "1701\n",
      "1702\n",
      "1703\n",
      "1704\n",
      "1705\n",
      "1706\n",
      "1707\n",
      "1708\n",
      "1709\n",
      "1710\n",
      "1711\n",
      "1712\n",
      "1713\n",
      "1714\n",
      "1715\n",
      "1716\n",
      "1717\n",
      "1718\n",
      "1719\n",
      "1720\n",
      "1721\n",
      "1722\n",
      "1723\n",
      "1724\n",
      "1725\n",
      "1726\n",
      "1727\n",
      "1728\n",
      "1729\n",
      "1730\n",
      "1731\n",
      "1732\n",
      "1733\n",
      "1734\n",
      "1735\n",
      "1736\n",
      "1737\n",
      "1738\n",
      "1739\n",
      "1740\n",
      "1741\n",
      "1742\n",
      "1743\n",
      "1744\n",
      "1745\n",
      "1746\n",
      "1747\n",
      "1748\n",
      "1749\n",
      "1750\n",
      "1751\n",
      "1752\n",
      "1753\n",
      "1754\n",
      "1755\n",
      "1756\n",
      "1757\n",
      "1758\n",
      "1759\n",
      "1760\n",
      "1761\n",
      "1762\n",
      "1763\n",
      "1764\n",
      "1765\n",
      "1766\n",
      "1767\n",
      "1768\n",
      "1769\n",
      "1770\n",
      "1771\n",
      "1772\n",
      "1773\n",
      "1774\n",
      "1775\n",
      "1776\n",
      "1777\n",
      "1778\n",
      "1779\n",
      "1780\n",
      "1781\n",
      "1782\n",
      "1783\n",
      "1784\n",
      "1785\n",
      "1786\n",
      "1787\n",
      "1788\n",
      "1789\n",
      "1790\n",
      "1791\n",
      "1792\n",
      "1793\n",
      "1794\n",
      "1795\n",
      "1796\n",
      "1797\n",
      "1798\n",
      "1799\n",
      "1800\n",
      "1801\n",
      "1802\n",
      "1803\n",
      "1804\n",
      "1805\n",
      "1806\n",
      "1807\n",
      "1808\n",
      "1809\n",
      "1810\n",
      "1811\n",
      "1812\n",
      "1813\n",
      "1814\n",
      "1815\n",
      "1816\n",
      "1817\n",
      "1818\n",
      "1819\n",
      "1820\n",
      "1821\n",
      "1822\n",
      "1823\n",
      "1824\n",
      "1825\n",
      "1826\n",
      "1827\n",
      "1828\n",
      "1829\n",
      "1830\n",
      "1831\n",
      "1832\n",
      "1833\n",
      "1834\n",
      "1835\n",
      "1836\n",
      "1837\n",
      "1838\n",
      "1839\n",
      "1840\n",
      "1841\n",
      "1842\n",
      "1843\n",
      "1844\n",
      "1845\n",
      "1846\n",
      "1847\n",
      "1848\n",
      "1849\n",
      "1850\n",
      "1851\n",
      "1852\n",
      "1853\n",
      "1854\n",
      "1855\n",
      "1856\n",
      "1857\n",
      "1858\n",
      "1859\n",
      "1860\n",
      "1861\n",
      "1862\n",
      "1863\n",
      "1864\n",
      "1865\n",
      "1866\n"
     ]
    }
   ],
   "source": [
    "\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "for l in links_vanhoa1[1674:]: \n",
    "  driver.get(l)    \n",
    "  time.sleep(2)\n",
    "  soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "  content = None\n",
    "  try:\n",
    "    all_p = soup.find('div', class_= \"art-body\").find_all(\"p\")\n",
    "  except:\n",
    "    pass\n",
    "  content = \" \".join([x.text for x in all_p if len(x.text.split()) > 10])\n",
    "  label = \"Văn hóa\"\n",
    "  labels_vanhoa.append(label)\n",
    "  contents_vanhoa.append(content)\n",
    "  links_vanhoa.append(l)\n",
    "  # print(len(labels))\n",
    "  # try:\n",
    "  #   driver.get(l)    \n",
    "  #   time.sleep(2)\n",
    "  #   soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "  #   all_p = soup.find('div', class_= \"content-detail textview\").find_all(\"p\")\n",
    "  #   content = \" \".join([x.text for x in all_p if len(x.text.split()) > 10])\n",
    "  #   label = \"Khoa học\"\n",
    "  # except:\n",
    "  #   driver.quit()\n",
    "  #   driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "  #   content = \"\"\n",
    "  #   label = \"\"\n",
    "  # labels.append(label)\n",
    "  # contents.append(content)\n",
    "  # links.append(l)\n",
    "  print(len(labels_vanhoa))\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vanhoa = pd.DataFrame({\"Link\": links_vanhoa, \"Content\": contents_vanhoa, \"Label\": labels_vanhoa})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chủ đề chương trình tập trung phản ánh những hoạt động của ngành Công an học tập và làm theo tấm gương tư tưởng, đạo đức phong cách của Chủ tịch Hồ Chí Minh, cùng những hình ảnh tận tụy, hy sinh bảo vệ cuộc sống bình yên, hạnh phúc cho nhân dân, theo lời dạy của Bác. Sự hy sinh của những người con ưu tú đó là để sự sống hôm nay nảy mầm và đơm hoa kết trái và cũng là động lực để tiếp thêm sức mạnh cho đồng đội vững vàng, quyết tâm hơn trên mặt trận đầy cam go, khốc liệt bảo vệ bình yên cho nhân dân. Tham gia chương trình có nhiều nhân vật gồm các chiến sĩ công an xã tình nguyện về những bản làng heo hút nhất; các chiến sĩ cảnh sát giao thông xuất sắc trong công tác; các chiến sĩ phòng cháy chữa cháy làm nhiệm vụ trong nước và tham gia cứu hộ quốc tế ở Thổ Nhĩ Kỳ... Chương trình còn khắc họa hình ảnh các trinh sát gan góc của cuộc chiến chống ma túy, chống buôn người ở những vùng biên giới xa xôi; các chiến sĩ giúp người dân làm nhà, dạy học, và lãnh đạo công an các tỉnh thành có nhiều đóng góp vào những chiến công của lực lượng Công an. Nhiều câu chuyện ấn tượng sẽ được kể lại trong chương trình. Đó là câu chuyện sản phụ đẻ rơi trên đường được hai cảnh sát giao thông dẫn đường đến bệnh viện cấp cứu; đó là nữ trinh sát miền cực Nam Tổ quốc 20 năm gắn bó với nghề, hy sinh cả hạnh phúc riêng tư; đó là những cảnh sát “khi dân chạy ra thì chúng tôi lao vào” trong các vụ cháy kinh hoàng- những người hùng giữa biển lửa… Điều đặc biệt là kịch bản chương trình sẽ tạo ra những nút thắt táo bạo, khi những người dân từng gặp nạn gặp lại các chiến sĩ- ân nhân của mình, chắc chắn sẽ là những giây phút xúc động vì vẻ đẹp của tình người, tình đồng loại, tình quân dân. Những ca khúc đi cùng năm tháng: Tiến bước dưới quân kỳ, Bác Hồ một tình yêu bao la, Anh công an về bản, Bình yên những nẻo đường, Khát vọng xanh, Áo mới Cà Mau… được xử lý hiện đại, thể hiện mới mẻ qua giọng ca của các nghệ sĩ thuộc nhiều thế hệ khác nhau và các các tiết mục ca múa cộng đồng của người Lô Lô được biên đạo tinh tế sẽ mang tới cho khán giả nhiều cung bậc cảm xúc về chiến công của các chiến sĩ công an làm theo 6 điều Bác Hồ dạy. Họ như những bông hoa rực rỡ nở giữa tháng 5 dâng lên Chủ tịch Hồ Chí Minh trong kỷ niệm ngày sinh của Người.'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vanhoa.iloc[1500].Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vanhoa.to_csv(\"vanhoa.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
