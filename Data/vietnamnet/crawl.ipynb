{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "domain = \"https://vietnamnet.vn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "# driver.get(\"vnnnhghhghhttps://vietnamnet.vn/thu-tuong-pham-minh-chinh-cung-thu-tuong-luxembourg-danh-trong-sam-tai-van-mieu-2139369.html\")\n",
    "# soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "# import requests\n",
    "# requests.get(\"vnnnhghhghhttps://vietnamnet.vn/thu-tuong-pham-minh-chinh-cung-thu-tuong-luxembourg-danh-trong-sam-tai-van-mieu-2139369.html\").status_code\n",
    "# content_element = soup.find(\"div\", class_= \"maincontent\")\n",
    "# arr_tag_content = []\n",
    "# try: \n",
    "#   for i in content_element.find_all(\"p\"):\n",
    "#     arr_tag_content.append(i.text)\n",
    "#   # if(len(contents) != url_news.index(link) + 1):\n",
    "#   #   print(link)\n",
    "#   #   print(content_string)\n",
    "# except Exception as e:\n",
    "#   print(\"fail to add content\" + str(e))\n",
    "\n",
    "# content_string = \" \".join(arr_tag_content)\n",
    "# driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls_in_pages(driver, page_urls):\n",
    "  url_news = []\n",
    "  for page_url in page_urls:\n",
    "    driver.get(page_url)\n",
    "    time.sleep(1)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    tag_elements = soup.find_all('h3', class_= 'horizontalPost__main-title')\n",
    "    a_elements = [tag.find(\"a\") for tag in tag_elements]\n",
    "    for a in a_elements:\n",
    "      try:\n",
    "        if domain in a['href']:\n",
    "          url_news.append(a['href'])\n",
    "        else:\n",
    "          rl_news.append(domain + a['href'])\n",
    "      except:\n",
    "        pass\n",
    "  return url_news\n",
    "\n",
    "def get_content_and_label_from_link(driver, url_news, label):\n",
    "  contents = []\n",
    "  labels = []\n",
    "  for link in url_news:\n",
    "    driver.get(link)\n",
    "    time.sleep(1)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "     \n",
    "    content_element = soup.find(\"div\", class_= \"maincontent\")\n",
    "    arr_tag_content = []\n",
    "    try: \n",
    "      for i in content_element.find_all(\"p\"):\n",
    "        arr_tag_content.append(i.text)\n",
    "    except Exception as e: \n",
    "      print(\"content faild\")\n",
    "      print(e)\n",
    "      print(link)\n",
    "    content_string = \" \".join(arr_tag_content)\n",
    "    contents.append(content_string)\n",
    "    labels.append(label)\n",
    "    if(len(contents) != url_news.index(link) + 1):\n",
    "      break\n",
    "    \n",
    "  return contents, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = []\n",
    "for index in range(1, 50):\n",
    "  uri = \"/thoi-su/chinh-tri-page\"\n",
    "  pages.append(domain + uri + str(index))\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "url_title = get_urls_in_pages(driver, pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl(uri, label):\n",
    "  # uri = \"/thoi-su/chinh-tri-page\"\n",
    "  pages = []\n",
    "  for index in range(1, 3):\n",
    "    pages.append(domain + uri + str(index))\n",
    "  driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "  url_title = get_urls_in_pages(driver, pages)\n",
    "  time.sleep(5)\n",
    "  contents, labels = get_content_and_label_from_link(driver, url_title, label)\n",
    "  driver.quit()\n",
    "  print(len(url_title), len(contents), len(labels))\n",
    "  data = pd.DataFrame({\"Link\": url_title, \"Contents\": contents, \"Labels\": labels})\n",
    "  data.to_csv(\"data_v1_\"+label+\".csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawl(\"/thoi-su/chinh-tri-page\", \"politics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"data_v1_politics.csv\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of URLs to be processed\n",
    "urls = ['/thoi-su/chinh-tri-page', '/kinh-doanh-page', '/the-thao-page', '/suc-khoe-page', '/giao-duc-page', '/giai-tri-page']\n",
    "labels = ['politics', 'business', 'sport', 'health', 'entertainment', 'education']\n",
    "# Create a ThreadPoolExecutor with 3 worker threads\n",
    "with ThreadPoolExecutor(max_workers=6) as executor:\n",
    "    # Submit a task for each URL in the list\n",
    "    futures = [executor.submit(crawl, url, labels) for url,label in zip(urls, labels)]\n",
    "\n",
    "    # Wait for all tasks to complete and print the results\n",
    "    for i in futures:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "def get_title(url):\n",
    "    driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "    driver.get(url)\n",
    "    title = driver.title\n",
    "    driver.quit()\n",
    "    return title\n",
    "\n",
    "# create a new thread and start it\n",
    "thread = threading.Thread(target=get_title, args=(\"Hello\", \"World\"))\n",
    "thread.start()\n",
    "\n",
    "# wait for the thread to finish\n",
    "thread.join()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
